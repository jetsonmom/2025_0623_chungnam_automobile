{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOR7EzKl+uEnLJ12ebG2RBl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jetsonmom/2025_0623_chungnam_automobile/blob/main/cuda%26ufunc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "nVDHnEc0M6PE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numba\n",
        "from numba import cuda\n",
        "\n",
        "if cuda.is_available():\n",
        "  print(\"üéâ ÏÑ±Í≥µ: NumbaÍ∞Ä GPUÎ•º ÏÑ±Í≥µÏ†ÅÏúºÎ°ú Ïù∏ÏãùÌñàÏäµÎãàÎã§!\")\n",
        "  print(\"Ïù¥Ï†ú ufunc Îì± Î™®Îì† CUDA ÏΩîÎìúÎ•º Ïã§ÌñâÌï† Ïàò ÏûàÏäµÎãàÎã§.\")\n",
        "  print(\"-\" * 50)\n",
        "  # NumbaÍ∞Ä Ïù∏ÏãùÌïú GPUÏùò ÏÉÅÏÑ∏ Ï†ïÎ≥¥Î•º Ï∂úÎ†•Ìï©ÎãàÎã§.\n",
        "  cuda.detect()\n",
        "else:\n",
        "  # Í±∞Ïùò Î∞úÏÉùÌïòÏßÄ ÏïäÍ≤†ÏßÄÎßå, ÎßåÏïΩÏùÑ ÏúÑÌïú Ïò§Î•ò Î©îÏãúÏßÄ\n",
        "  print(\"Ïò§Î•ò: ÏïÑÏßÅ NumbaÍ∞Ä GPUÎ•º Ïù∏ÏãùÌïòÏßÄ Î™ªÌï©ÎãàÎã§. Îü∞ÌÉÄÏûÑÏùÑ Îã§Ïãú Ï¥àÍ∏∞ÌôîÌï¥Î≥¥ÏÑ∏Ïöî.\")"
      ],
      "metadata": {
        "id": "3mcu5mjhO8rx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from numba import cuda\n",
        "\n",
        "if cuda.is_available():\n",
        "  print(\"ÏÑ±Í≥µ! NumbaÍ∞Ä GPUÎ•º Ïù∏ÏãùÌñàÏäµÎãàÎã§.\")\n",
        "  print(\"GPU Ïû•Ïπò Ï†ïÎ≥¥:\")\n",
        "  cuda.detect()\n",
        "else:\n",
        "  print(\"Ïò§Î•ò: NumbaÍ∞Ä GPUÎ•º Ïù∏ÏãùÌïòÏßÄ Î™ªÌñàÏäµÎãàÎã§.\")"
      ],
      "metadata": {
        "id": "o4QDVUyMPZLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numba import vectorize\n",
        "\n",
        "# ufunc Ï†ïÏùò: Í∞Å ÏûÖÎ†•Í∞í(x)Ïóê 2Î•º Í≥±ÌïòÍ≥† 1ÏùÑ ÎçîÌïòÎäî Ìï®Ïàò\n",
        "# target='cuda' ÏòµÏÖòÏúºÎ°ú Ïù¥ Ìï®ÏàòÎ•º GPUÏóêÏÑú Ïã§ÌñâÌïòÎèÑÎ°ù ÏßÄÏ†ïÌï©ÎãàÎã§.\n",
        "@vectorize(['float32(float32)'], target='cuda')\n",
        "def add_two_and_one(x):\n",
        "  return x * 2 + 1\n",
        "\n",
        "# 0Î∂ÄÌÑ∞ 9ÍπåÏßÄÏùò Ïà´ÏûêÎ°ú Î∞∞Ïó¥ ÏÉùÏÑ±\n",
        "data = np.arange(10, dtype=np.float32)\n",
        "\n",
        "print(\"ÏõêÎ≥∏ Îç∞Ïù¥ÌÑ∞:\", data)\n",
        "\n",
        "# GPUÏóêÏÑú ufunc Ïã§Ìñâ\n",
        "result = add_two_and_one(data)\n",
        "\n",
        "print(\"GPU Ïó∞ÏÇ∞ Í≤∞Í≥º:\", result)"
      ],
      "metadata": {
        "id": "EVThQvTkPf2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o47SPxCyGCWW"
      },
      "outputs": [],
      "source": [
        "## See: https://github.com/googlecolab/colabtools/issues/5081#issuecomment-2629611179\n",
        "!uv pip install -q --system numba-cuda==0.4.0\n",
        "from numba import config\n",
        "config.CUDA_ENABLE_PYNVJITLINK = 1\n",
        "config.CUDA_LOW_OCCUPANCY_WARNINGS = 0"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s5Tub08jJrja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numba\n",
        "import numpy as np\n",
        "from numba import vectorize, jit, cuda\n",
        "print(\"Numpy version: \", np.__version__)\n",
        "print(\"numba version: \", numba.__version__)\n",
        "print(\"Cuda avilable\") if cuda.detect() else print(\"Cuda not avilable\")"
      ],
      "metadata": {
        "id": "2xM12X8yG7wl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Day 5: CUDA ÌååÏù¥Ïç¨ÏùÑ Ïù¥Ïö©Ìïú ÏûêÏú®Ï£ºÌñâ Îç∞Ïù¥ÌÑ∞ Í∞ÄÏÜç\n",
        "\n",
        "**Í≥ºÏ†ï Î™©Ìëú:** ÏßÄÎÇú 4ÏùºÍ∞Ñ Î∞∞Ïö¥ ÏûêÏú®Ï£ºÌñâ Îç∞Ïù¥ÌÑ∞ Ï≤òÎ¶¨ ÏïåÍ≥†Î¶¨Ï¶òÎì§ÏùÑ GPUÎ•º Ïù¥Ïö©Ìï¥ Í∞ÄÏÜçÌïòÎäî Î∞©Î≤ïÏùÑ Î∞∞ÏõÅÎãàÎã§. Numba ÎùºÏù¥Î∏åÎü¨Î¶¨Î•º ÏÇ¨Ïö©ÌïòÏó¨ PythonÏúºÎ°ú ÏßÅÏ†ë CUDA ÏΩîÎìúÎ•º ÏûëÏÑ±ÌïòÍ≥†, Î≥ëÎ†¨ Ï≤òÎ¶¨Ïùò ÌïµÏã¨ ÏõêÎ¶¨ÏôÄ ÏµúÏ†ÅÌôî Í∏∞Î≤ïÏùÑ Ïã§ÏäµÏùÑ ÌÜµÌï¥ ÏùµÌûôÎãàÎã§."
      ],
      "metadata": {
        "id": "mIpouIJWHH2d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Í≥ºÏ†ï Î™©Ìëú:** ÏßÄÎÇú 4ÏùºÍ∞Ñ Î∞∞Ïö¥ ÏûêÏú®Ï£ºÌñâ Îç∞Ïù¥ÌÑ∞ Ï≤òÎ¶¨ ÏïåÍ≥†Î¶¨Ï¶òÎì§ÏùÑ GPUÎ•º Ïù¥Ïö©Ìï¥ Í∞ÄÏÜçÌïòÎäî Î∞©Î≤ïÏùÑ Î∞∞ÏõÅÎãàÎã§. Numba ÎùºÏù¥Î∏åÎü¨Î¶¨Î•º ÏÇ¨Ïö©ÌïòÏó¨ PythonÏúºÎ°ú ÏßÅÏ†ë CUDA ÏΩîÎìúÎ•º ÏûëÏÑ±ÌïòÍ≥†, Î≥ëÎ†¨ Ï≤òÎ¶¨Ïùò ÌïµÏã¨ ÏõêÎ¶¨ÏôÄ ÏµúÏ†ÅÌôî Í∏∞Î≤ïÏùÑ Ïã§ÏäµÏùÑ ÌÜµÌï¥ ÏùµÌûôÎãàÎã§.\n",
        "---\n",
        "## Lab 1: Numba Ufunc, ÌîÑÎ°úÌååÏùºÎßÅ, Í∑∏Î¶¨Í≥† Ï†ïÎ∞ÄÎèÑ\n",
        "\n",
        "**Ïã§Ïäµ Î™©Ìëú:**\n",
        "1. Í∞ÑÎã®Ìïú Ïù¥ÎØ∏ÏßÄ Ï≤òÎ¶¨ ÏûëÏóÖÏùÑ ÏúÑÌïú GPU Î≤îÏö© Ìï®Ïàò(ufunc)Î•º ÏûëÏÑ±Ìï©ÎãàÎã§.\n",
        "2. Îç∞Ïù¥ÌÑ∞ ÌÅ¨Í∏∞Ïóê Îî∞Î•∏ CPUÏôÄ GPUÏùò ÏÑ±Îä•ÏùÑ `%%timeit`ÏúºÎ°ú ÎπÑÍµêÌïòÏó¨ Î©îÎ™®Î¶¨ Ï†ÑÏÜ° Ïò§Î≤ÑÌó§ÎìúÎ•º ÌôïÏù∏Ìï©ÎãàÎã§.\n",
        "3. `float32`ÏôÄ `float64`Ïùò Ï†ïÎ∞ÄÎèÑ Ï∞®Ïù¥Í∞Ä Í≥ÑÏÇ∞ Í≤∞Í≥ºÏóê ÎØ∏ÏπòÎäî ÏòÅÌñ•ÏùÑ ÏßÅÏ†ë ÌôïÏù∏Ìï©ÎãàÎã§.\n",
        "### Ï§ÄÎπÑ: ÎùºÏù¥Î∏åÎü¨Î¶¨ ÏûÑÌè¨Ìä∏ Î∞è Îç∞Ïù¥ÌÑ∞ ÏÉùÏÑ±"
      ],
      "metadata": {
        "id": "v6EQblACHcLy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZtEB0v9ZHJhB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numba import vectorize, jit\n",
        "import time\n",
        "\n",
        "# ÏûêÏú®Ï£ºÌñâ Ïπ¥Î©îÎùº Ïù¥ÎØ∏ÏßÄÎ•º Î™®Î∞©Ìïú Í∞ÄÏÉÅ Îç∞Ïù¥ÌÑ∞ ÏÉùÏÑ±\n",
        "def create_image(size_mb):\n",
        "    # 1 pixel = 1 byte (uint8)\n",
        "    num_pixels = size_mb * 1024 * 1024\n",
        "    # Ïù¥ÎØ∏ÏßÄÏùò Í∞ÄÎ°ú:ÏÑ∏Î°ú ÎπÑÏú®ÏùÑ 16:9Î°ú Í∞ÄÏ†ï\n",
        "    height = int(np.sqrt(num_pixels * 9 / 16))\n",
        "    width = int(height * 16 / 9)\n",
        "    print(f\"ÏÉùÏÑ±Îêú Ïù¥ÎØ∏ÏßÄ ÌÅ¨Í∏∞: {width}x{height} ({width*height/1024/1024:.2f} MB)\")\n",
        "    return np.random.randint(0, 256, size=(height, width), dtype=np.uint8)\n",
        "\n",
        "small_image = create_image(1)    # ÏïΩ 1MB ÌÅ¨Í∏∞ Ïù¥ÎØ∏ÏßÄ\n",
        "large_image = create_image(100)  # ÏïΩ 100MB ÌÅ¨Í∏∞ Ïù¥ÎØ∏ÏßÄ"
      ],
      "metadata": {
        "id": "i0qMYhtIHgsp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "mtCKL-0oIMWC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: ÏïÑÎûò Ìï®ÏàòÎ•º NumbaÏùò @vectorize Îç∞ÏΩîÎ†àÏù¥ÌÑ∞Î•º ÏÇ¨Ïö©ÌïòÏó¨ GPU ufuncÏúºÎ°ú Î≥ÄÌôòÌïòÏÑ∏Ïöî.\n",
        "# targetÏùÑ 'cuda'Î°ú ÏÑ§Ï†ïÌïòÎäî Í≤ÉÏùÑ ÏûäÏßÄ ÎßàÏÑ∏Ïöî.\n",
        "# ÌÉÄÏûÖ ÏãúÍ∑∏ÎãàÏ≤ò: uint8Î•º ÏûÖÎ†•Î∞õÏïÑ uint8Î•º Î∞òÌôò -> ['uint8(uint8, uint8)']\n",
        "\n",
        "from numba import vectorize, uint8, cuda\n",
        "\n",
        "@vectorize([uint8(uint8, uint8)], target='cuda')\n",
        "def gpu_threshold(pixel, threshold):\n",
        "    return 255 if pixel > threshold else 0\n",
        "\n",
        "# CPU Î≤ÑÏ†Ñ (NumPy)\n",
        "def cpu_threshold_numpy(image, threshold):\n",
        "    return np.where(image > threshold, 255, 0).astype(np.uint8)"
      ],
      "metadata": {
        "id": "TeoYn3FlI71i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- ÏûëÏùÄ Ïù¥ÎØ∏ÏßÄ (1MB) ÏÑ±Îä• ÎπÑÍµê ---\")\n",
        "print(\"CPU (NumPy):\")\n",
        "%timeit cpu_threshold_numpy(small_image, 128)\n",
        "\n",
        "print(\"\\nGPU (Numba ufunc):\")\n",
        "# Ï≤´ Ïã§ÌñâÏùÄ Ïª¥ÌååÏùº ÏãúÍ∞Ñ Ìè¨Ìï®\n",
        "_ = gpu_threshold(small_image, 128)\n",
        "%timeit gpu_threshold(small_image, 128)\n",
        "\n"
      ],
      "metadata": {
        "id": "OrKZPMrbJAGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- ÌÅ∞ Ïù¥ÎØ∏ÏßÄ (100MB) ÏÑ±Îä• ÎπÑÍµê ---\")\n",
        "print(\"CPU (NumPy):\")\n",
        "%timeit cpu_threshold_numpy(large_image, 128)\n",
        "\n",
        "print(\"\\nGPU (Numba ufunc):\")\n",
        "_ = gpu_threshold(large_image, 128)\n",
        "%timeit gpu_threshold(large_image, 128)"
      ],
      "metadata": {
        "id": "gSo5oMjCJDJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numba import vectorize\n",
        "\n",
        "# Corrected version to demonstrate precision loss\n",
        "@vectorize(['float32(float32)'], target='cuda')\n",
        "def precision_test_f32_corrected(x):\n",
        "    # Force the literal '1.0' to be a 32-bit float\n",
        "    one_f32 = np.float32(1.0)\n",
        "    return (one_f32 + x) - one_f32\n",
        "\n",
        "@vectorize(['float64(float64)'], target='cuda')\n",
        "def precision_test_f64(x):\n",
        "    # For float64, using a Python literal is fine as it's already 64-bit\n",
        "    return (1.0 + x) - 1.0\n",
        "\n",
        "# Using a value that WILL be lost in float32 but not float64\n",
        "# float32 machine epsilon is ~1.19e-7. 1e-8 is smaller than that.\n",
        "val = 1e-8\n",
        "x_f32 = np.array([val], dtype=np.float32)\n",
        "x_f64 = np.array([val], dtype=np.float64)\n",
        "\n",
        "# Run the corrected f32 version and the f64 version\n",
        "result_f32 = precision_test_f32_corrected(x_f32)\n",
        "result_f64 = precision_test_f64(x_f64)\n",
        "\n",
        "print(f\"ÏûÖÎ†• Í∞í: {val}\")\n",
        "print(f\"Corrected Float32 Í≤∞Í≥º: {result_f32[0]}\")\n",
        "print(f\"Float64 Í≤∞Í≥º: {result_f64[0]}\")"
      ],
      "metadata": {
        "id": "QDDmFnKfJG5y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}